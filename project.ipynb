{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamlitSecretNotFoundError",
     "evalue": "No secrets found. Valid paths for a secrets.toml file or secret directories are: C:\\Users\\LENOVO\\.streamlit\\secrets.toml, c:\\Users\\LENOVO\\Documents\\1.Personal\\0.Projects\\Github\\IIT-Kharagpur-AI4ICPS-Certificate-Programme\\HAAI++\\Week-9\\.streamlit\\secrets.toml",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamlitSecretNotFoundError\u001b[39m              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m OLLAMA_URL = \u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43msecrets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOLLAMA_URL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost:11434\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m MODEL = st.secrets.get(\u001b[33m\"\u001b[39m\u001b[33mOLLAMA_MODEL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgemma-3-1b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ---------------------- Utilities: extraction ----------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:811\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Documents\\1.Personal\\0.Projects\\Virtual_Environments\\.ds_venv\\Lib\\site-packages\\streamlit\\runtime\\secrets.py:476\u001b[39m, in \u001b[36mSecrets.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the value with the given key. If no such key\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[33;03mexists, raise a KeyError.\u001b[39;00m\n\u001b[32m    472\u001b[39m \n\u001b[32m    473\u001b[39m \u001b[33;03mThread-safe.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[key]\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Mapping):\n\u001b[32m    478\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Documents\\1.Personal\\0.Projects\\Virtual_Environments\\.ds_venv\\Lib\\site-packages\\streamlit\\runtime\\secrets.py:377\u001b[39m, in \u001b[36mSecrets._parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found_secrets_file:\n\u001b[32m    372\u001b[39m     error_msg = (\n\u001b[32m    373\u001b[39m         secret_error_messages_singleton.get_no_secrets_found_message(\n\u001b[32m    374\u001b[39m             file_paths\n\u001b[32m    375\u001b[39m         )\n\u001b[32m    376\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StreamlitSecretNotFoundError(error_msg)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m secrets.items():\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_set_environment_variable(k, v)\n",
      "\u001b[31mStreamlitSecretNotFoundError\u001b[39m: No secrets found. Valid paths for a secrets.toml file or secret directories are: C:\\Users\\LENOVO\\.streamlit\\secrets.toml, c:\\Users\\LENOVO\\Documents\\1.Personal\\0.Projects\\Github\\IIT-Kharagpur-AI4ICPS-Certificate-Programme\\HAAI++\\Week-9\\.streamlit\\secrets.toml"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Streamlit app: CV Creation using LLMs (minimal, runnable single-file demo)\n",
    "\n",
    "Features included:\n",
    "- Upload PDF or DOCX resume -> extract text\n",
    "- Paste job description\n",
    "- Call local Ollama (Gemma 3 1B) for: resume extraction JSON, JD parsing, tailoring\n",
    "- Create a DOCX of the tailored resume and provide download link\n",
    "\n",
    "Requirements (install):\n",
    "pip install streamlit pdfplumber python-docx requests pydantic\n",
    "\n",
    "Notes:\n",
    "- Assumes Ollama running locally at http://localhost:11434 and model 'gemma-3-1b' available.\n",
    "- This is a minimal demo for capstone work; replace prompt templates & safety checks in production.\n",
    "\n",
    "Run: streamlit run streamlit_cv_llm_demo.py\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "OLLAMA_URL = st.secrets.get(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "MODEL = st.secrets.get(\"OLLAMA_MODEL\", \"gemma-3-1b\")\n",
    "\n",
    "# ---------------------- Utilities: extraction ----------------------\n",
    "\n",
    "def pdf_to_text(file_bytes: bytes) -> str:\n",
    "    text_pages = []\n",
    "    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            text_pages.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(text_pages)\n",
    "\n",
    "\n",
    "def docx_to_text(file_bytes: bytes) -> str:\n",
    "    doc = Document(io.BytesIO(file_bytes))\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "\n",
    "# ---------------------- Ollama minimal client ----------------------\n",
    "\n",
    "def call_ollama(prompt: str, model: str = MODEL, max_tokens: int = 800) -> str:\n",
    "    \"\"\"Call Ollama's text generation API. Returns textual output (string).\n",
    "    Minimal POST to /api/generate per Ollama HTTP API conventions.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        # Ollama's API returns a `choices` list depending on version; handle common shapes\n",
    "        if \"choices\" in data and len(data[\"choices\"])>0:\n",
    "            # many Ollama versions put content in choices[0].content\n",
    "            c = data[\"choices\"][0]\n",
    "            if isinstance(c, dict) and \"message\" in c:\n",
    "                return c[\"message\"].get(\"content\",\"\")\n",
    "            return c.get(\"content\", \"\")\n",
    "        # fallbacks\n",
    "        return data.get(\"text\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_CALLING_OLLAMA: {e}\"\n",
    "\n",
    "\n",
    "# ---------------------- Prompts & LLM wrappers ----------------------\n",
    "\n",
    "RESUME_EXTRACTION_PROMPT = \"\"\"\n",
    "You are a JSON extractor. Given the text of a resume, extract and output only a valid JSON object with the fields:\n",
    "name, contact {email, phone}, location, summary, education [ {school, degree, start, end} ],\n",
    "experience [ {title, company, start, end, bullets} ], skills [list], projects [list], achievements [list].\n",
    "If a field is missing, return an empty string or empty list.\n",
    "Resume text:\n",
    "\"\"\"\n",
    "\n",
    "JD_EXTRACTION_PROMPT = \"\"\"\n",
    "You are a job description parser. Given the job description text, extract and output only a valid JSON object with fields:\n",
    "job_title, seniority, location, required_skills (list), preferred_skills (list), responsibilities (list), keywords (list).\n",
    "Job description:\n",
    "\"\"\"\n",
    "\n",
    "TAILOR_PROMPT = \"\"\"\n",
    "You are an ATS-aware resume writer. Given resume JSON and job JSON (below), rewrite the resume JSON to align better with the job.\n",
    "- Emphasize measurable achievements, use active verbs, and include keywords from the job where relevant.\n",
    "- Do NOT invent facts; if unsure, leave original text.\n",
    "- Output only the updated resume JSON.\n",
    "\n",
    "Resume JSON:\n",
    "{resume_json}\n",
    "\n",
    "Job JSON:\n",
    "{job_json}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_resume_json(resume_text: str) -> Dict[str, Any]:\n",
    "    prompt = RESUME_EXTRACTION_PROMPT + \"\\n\" + resume_text\n",
    "    out = call_ollama(prompt)\n",
    "    # try to parse returned JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(out)\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        # attempt to extract json substring\n",
    "        start = out.find('{')\n",
    "        end = out.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            try:\n",
    "                return json.loads(out[start:end+1])\n",
    "            except Exception:\n",
    "                return {\"error\":\"could_not_parse_llm_output\",\"raw\":out}\n",
    "        return {\"error\":\"no_json\",\"raw\":out}\n",
    "\n",
    "\n",
    "def parse_jd(jd_text: str) -> Dict[str, Any]:\n",
    "    prompt = JD_EXTRACTION_PROMPT + \"\\n\" + jd_text\n",
    "    out = call_ollama(prompt)\n",
    "    try:\n",
    "        return json.loads(out)\n",
    "    except Exception:\n",
    "        start = out.find('{')\n",
    "        end = out.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            try:\n",
    "                return json.loads(out[start:end+1])\n",
    "            except Exception:\n",
    "                return {\"error\":\"could_not_parse_jd\",\"raw\":out}\n",
    "        return {\"error\":\"no_json\",\"raw\":out}\n",
    "\n",
    "\n",
    "def tailor_resume(resume_json: Dict[str, Any], job_json: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    prompt = TAILOR_PROMPT.format(resume_json=json.dumps(resume_json, ensure_ascii=False), job_json=json.dumps(job_json, ensure_ascii=False))\n",
    "    out = call_ollama(prompt, max_tokens=1200)\n",
    "    try:\n",
    "        return json.loads(out)\n",
    "    except Exception:\n",
    "        start = out.find('{')\n",
    "        end = out.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            try:\n",
    "                return json.loads(out[start:end+1])\n",
    "            except Exception:\n",
    "                return {\"error\":\"could_not_parse_tailored\",\"raw\":out}\n",
    "        return {\"error\":\"no_json_tailored\",\"raw\":out}\n",
    "\n",
    "\n",
    "# ---------------------- Document generation ----------------------\n",
    "\n",
    "def create_docx(resume_json: Dict[str, Any]) -> bytes:\n",
    "    doc = Document()\n",
    "    name = resume_json.get('name','')\n",
    "    doc.add_heading(name, level=0)\n",
    "    contact = resume_json.get('contact', {})\n",
    "    contact_line = []\n",
    "    if contact.get('email'): contact_line.append(contact.get('email'))\n",
    "    if contact.get('phone'): contact_line.append(contact.get('phone'))\n",
    "    if resume_json.get('location'): contact_line.append(resume_json.get('location'))\n",
    "    if contact_line:\n",
    "        doc.add_paragraph(' | '.join(contact_line))\n",
    "    if resume_json.get('summary'):\n",
    "        doc.add_heading('Summary', level=1)\n",
    "        doc.add_paragraph(resume_json.get('summary'))\n",
    "\n",
    "    if resume_json.get('education'):\n",
    "        doc.add_heading('Education', level=1)\n",
    "        for edu in resume_json.get('education'):\n",
    "            line = f\"{edu.get('degree','')} — {edu.get('school','')}\"\n",
    "            dates = f\" ({edu.get('start','')} - {edu.get('end','')})\" if (edu.get('start') or edu.get('end')) else ''\n",
    "            p = doc.add_paragraph()\n",
    "            p.add_run(line).bold = True\n",
    "            p.add_run(dates)\n",
    "\n",
    "    if resume_json.get('experience'):\n",
    "        doc.add_heading('Experience', level=1)\n",
    "        for exp in resume_json.get('experience'):\n",
    "            p = doc.add_paragraph()\n",
    "            title_company = f\"{exp.get('title','')} — {exp.get('company','')}\"\n",
    "            p.add_run(title_company).bold = True\n",
    "            if exp.get('start') or exp.get('end'):\n",
    "                p.add_run(f\" ({exp.get('start','')} - {exp.get('end','')})\")\n",
    "            bullets = exp.get('bullets', []) or []\n",
    "            for b in bullets:\n",
    "                doc.add_paragraph(b, style='List Bullet')\n",
    "\n",
    "    if resume_json.get('skills'):\n",
    "        doc.add_heading('Skills', level=1)\n",
    "        doc.add_paragraph(', '.join(resume_json.get('skills')))\n",
    "\n",
    "    # save to bytes\n",
    "    f = io.BytesIO()\n",
    "    doc.save(f)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "# ---------------------- Streamlit UI ----------------------\n",
    "\n",
    "st.set_page_config(page_title=\"CV Creation using LLMs — Demo\", layout=\"wide\")\n",
    "st.title(\"CV Creation using LLMs — Minimal Demo\")\n",
    "\n",
    "col1, col2 = st.columns([1,2])\n",
    "\n",
    "with col1:\n",
    "    st.header(\"1) Upload resume\")\n",
    "    uploaded = st.file_uploader(\"Upload PDF or DOCX resume\", type=[\"pdf\",\"docx\"]) \n",
    "    if uploaded:\n",
    "        raw = uploaded.read()\n",
    "        if uploaded.type == \"application/pdf\" or uploaded.name.lower().endswith('.pdf'):\n",
    "            resume_text = pdf_to_text(raw)\n",
    "        else:\n",
    "            resume_text = docx_to_text(raw)\n",
    "        st.text_area(\"Extracted resume text (editable)\", value=resume_text, height=300, key='res_text')\n",
    "    else:\n",
    "        resume_text = st.session_state.get('res_text', '')\n",
    "\n",
    "    st.header(\"2) Paste job description\")\n",
    "    jd_text = st.text_area(\"Job description\", height=200)\n",
    "\n",
    "    run_extract = st.button(\"Extract Resume -> JSON\")\n",
    "    if run_extract:\n",
    "        if not resume_text.strip():\n",
    "            st.error(\"Please upload a resume first.\")\n",
    "        else:\n",
    "            with st.spinner(\"Calling local LLM to extract resume JSON...\"):\n",
    "                res_json = extract_resume_json(resume_text)\n",
    "                st.session_state['resume_json'] = res_json\n",
    "                st.success(\"Extraction complete — check JSON on the right.\")\n",
    "\n",
    "    run_parse_jd = st.button(\"Parse Job Description -> JSON\")\n",
    "    if run_parse_jd:\n",
    "        if not jd_text.strip():\n",
    "            st.error(\"Please paste job description text.\")\n",
    "        else:\n",
    "            with st.spinner(\"Parsing job description with LLM...\"):\n",
    "                job_json = parse_jd(jd_text)\n",
    "                st.session_state['job_json'] = job_json\n",
    "                st.success(\"Job parsed — check JSON on the right.\")\n",
    "\n",
    "    run_tailor = st.button(\"Tailor Resume to Job\")\n",
    "    if run_tailor:\n",
    "        if 'resume_json' not in st.session_state:\n",
    "            st.error(\"Run resume extraction first.\")\n",
    "        elif 'job_json' not in st.session_state:\n",
    "            st.error(\"Parse the job description first.\")\n",
    "        else:\n",
    "            with st.spinner(\"Generating tailored resume...\"):\n",
    "                tailored = tailor_resume(st.session_state['resume_json'], st.session_state['job_json'])\n",
    "                st.session_state['tailored_json'] = tailored\n",
    "                st.success(\"Tailoring complete — download on the right.\")\n",
    "\n",
    "with col2:\n",
    "    st.header(\"JSON outputs & Download\")\n",
    "    st.subheader(\"Extracted Resume JSON\")\n",
    "    st.code(json.dumps(st.session_state.get('resume_json', {}), indent=2, ensure_ascii=False))\n",
    "\n",
    "    st.subheader(\"Parsed Job JSON\")\n",
    "    st.code(json.dumps(st.session_state.get('job_json', {}), indent=2, ensure_ascii=False))\n",
    "\n",
    "    st.subheader(\"Tailored Resume JSON\")\n",
    "    st.code(json.dumps(st.session_state.get('tailored_json', {}), indent=2, ensure_ascii=False))\n",
    "\n",
    "    if st.session_state.get('tailored_json'):\n",
    "        st.download_button(\"Download tailored CV (DOCX)\", data=create_docx(st.session_state['tailored_json']), file_name=\"tailored_cv.docx\", mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**Tips & next steps:**\\n\\n- For production, add verification UI so users confirm dates and claims before finalizing.\\n- Replace zero-temp LLM calls with controlled prompting and add unit tests for parser outputs.\\n- Add ATS-simulated scoring and a lightweight UI to highlight missing keywords.\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"This demo calls a local Ollama instance. If you get errors referencing Ollama, make sure Ollama is running and the model is pulled. See app header comments for requirements.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ds_venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
